<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://lnnx2006.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lnnx2006.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-09-19T12:30:10+00:00</updated><id>https://lnnx2006.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Introduction to TensorFlow GNN</title><link href="https://lnnx2006.github.io/blog/2022/tfgnn-intro/" rel="alternate" type="text/html" title="Introduction to TensorFlow GNN" /><published>2022-06-23T00:00:00+00:00</published><updated>2022-06-23T00:00:00+00:00</updated><id>https://lnnx2006.github.io/blog/2022/tfgnn-intro</id><content type="html" xml:base="https://lnnx2006.github.io/blog/2022/tfgnn-intro/"><![CDATA[<p>For my first post in this blog, I’ve decided to write a beginner-level introduction to Graph Neural Networks with TensorFlow-GNN.
An interactive version of this post, where you can run and modify the code, is available as a <a href="https://www.kaggle.com/code/fidels/introduction-to-tf-gnn">Kaggle notebook</a> and also on <a href="https://colab.research.google.com/drive/1FQPBpdrd17WzX_rcFAMsIaN8EK0rP4Ai?usp=sharing">GoogleColab</a>. 
Please let me know if you find any bugs or errors, and also if you thought this was useful or you’d like more details about some particular point.</p>

<p><strong>[July 1, 2022 UPDATE]</strong> The <a href="https://www.kaggle.com/code/fidels/introduction-to-tf-gnn">Kaggle notebook</a> version of this post has been awarded the <a href="https://www.kaggle.com/google-oss-expert-prize">Google Open Source Expert Prize</a> for the month of July.</p>

<p><strong>[July 5, 2022 UPDATE]</strong> I’ve now updated this notebook to work with the latest TF-GNN release, v0.2.0; some typos were also corrected, and more resources have been added to <a href="#conclusions_resources">§5.1</a>.</p>

<h2 id="contents"><a id="contents">Contents</a></h2>

<ul>
  <li><a href="#introduction">1. Introduction</a>
    <ul>
      <li><a href="#introduction_graphs">1.1 Graphs and Graph Neural Networks</a></li>
      <li><a href="#introduction_tfgnn">1.2 TensorFlow GNN</a></li>
      <li><a href="#introduction_graph_classification">1.3 Graph classification</a></li>
      <li><a href="#introduction_dependencies">1.4 Dependencies and imports</a></li>
    </ul>
  </li>
  <li><a href="#dataprep">2. Data preparation</a>
    <ul>
      <li><a href="#dataprep_datasetprovider">2.1 The <code class="language-plaintext highlighter-rouge">DatasetProvider</code> protocol</a></li>
      <li><a href="#dataprep_inspection">2.2 Data inspection</a></li>
      <li><a href="#dataprep_batching">2.3 <code class="language-plaintext highlighter-rouge">GraphTensor</code> batching</a></li>
    </ul>
  </li>
  <li><a href="#mpnn">3. Vanilla MPNN models</a>
    <ul>
      <li><a href="#mpnn_embedding">3.1 Initial graph embedding</a></li>
      <li><a href="#mpnn_stack">3.2 Stacking message-passing layers</a></li>
      <li><a href="#mpnn_construction">3.3 Model construction</a></li>
    </ul>
  </li>
  <li><a href="#classification">4. Graph binary classification</a>
    <ul>
      <li><a href="#classification_task">4.1 Task specification</a></li>
      <li><a href="#classification_training">4.2 Training</a></li>
      <li><a href="#classification_metrics">4.3 Metric visualization</a></li>
    </ul>
  </li>
  <li><a href="#conclusions">5. Conclusions</a>
    <ul>
      <li><a href="#conclusions_resources">5.1 Resources and acknowledgements</a></li>
    </ul>
  </li>
</ul>

<div class="section separator"></div>

<h2 id="1-introduction"><a id="introduction">1. Introduction</a></h2>

<div class="subsection separator"></div>

<h3 id="11-graphs-and-graph-neural-networks"><a id="introduction_graphs">1.1 Graphs and Graph Neural Networks</a></h3>

<div class="text separator"></div>

<p>A <em><a href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)">graph</a></em> consists of a collection \(\mathcal{V}=\{1,\dots,n\}\) of <em>nodes</em>, sometimes also called <em>vertices</em>, and a set of <em>edges</em> \(\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}\). An edge \((i,j) \in \mathcal{E}\), also denoted \(i\to j\), represents a relation from node \(i\) to node \(j\) which in the general case is directed (<em>i.e.</em> does not necessarily imply the converse relation \(j\to i\)). <em>Heterogeneous</em> graphs can have different types of nodes and/or edges, and <em>decorated</em> graphs may have features associated to their nodes, \(h_i \in \mathbb{R}^{d_\mathcal{V}}\) for \(i=1,\dots,n\), and/or their edges, \(h_{i\to j} \in \mathbb{R}^{d_\mathcal{E}}\) for \((i,j)\in\mathcal{E}\).</p>

<p>Graphs are clearly a very general and powerful concept, which can be used to represent many different kinds of data. For example:</p>
<ul>
  <li>A social network may be thought of as a graph where the nodes correspond to different users, and the edges correspond to their relationships (<em>i.e.</em> “friendship”, “follower”, etc).</li>
  <li>A country may be represented as a graph where cities are considered to be nodes, with roads connecting them playing the role of edges.</li>
  <li>…</li>
</ul>

<hr />

<p><a href="https://distill.pub/2021/gnn-intro/">Graph Neural Networks (GNN)</a> are one way in which we can apply neural networks to graph-structured data in order to learn to make predictions about it. Their architecture typically involves stacking <em>message passing</em> layers, each of which updates the features of each node \(i\) in the graph by applying some function to the features of its neighbors, <em>i.e.</em> all the nodes \(j\) which have edges going from \(j\) to \(i\). More formally, a message passing layer implements the transformation</p>

\[h_i' = f_{\theta_\mathcal{V}}\left(h_i, \tilde{\sum}_{(j,i) \in \mathcal{E}} g_{\theta_\mathcal{E}}\left(h_j, h_{j\to i}\right)\right) \qquad\text{for}\qquad i = 1, \dots, n,\]

<p>where \(f_{\theta_\mathcal{V}}\) is a neural network applied at each node with parameters \(\theta_\mathcal{V}\), and \(g_{\theta_\mathcal{E}}\) is a neural network applied at each edge with parameters \(\theta_\mathcal{E}\). Note that the latter may be trivial if edges do not have features. In the formula above \(\tilde\sum\) in fact represents <em>any</em> orderless aggregation function, which can be a conventional summation, an averaging operation, etc. The key point is that, being orderless, a GNN constructed in this way will be invariant under the permutation of the node labels \(i=1,\dots,n\), a property which of course should hold since the labels themselves are generally speaking arbitrary: the information is encoded in the graph structure itself, not the particular labels chosen to represent it.</p>

<h3 id="12-tensorflow-gnn"><a id="introduction_tfgnn">1.2 TensorFlow GNN</a></h3>

<p><a href="https://github.com/tensorflow/gnn">TensorFlow GNN (TF-GNN)</a> is a fairly recent addition to the TensorFlow ecosystem, having been first released in late 2021. At this time there are relatively few resources available explaining how to use TF-GNN in practice (see <a href="#conclusions_resources">§5.1</a> for more); this notebook attempts to start filling the gap by providing a basic example of the application of TF-GNN to a real-world problem, namely the classification of molecules to detect cardiotoxicity.</p>

<p>TF-GNN is very powerful and flexible, and can deal with large, heterogeneous graphs. We will however keep things simple here, and work instead with rather small, homogeneous graphs having few features. The main focus will be on showcasing the basic concepts and design abstractions introduced in TF-GNN to facilitate and accelerate end-to-end training of GNN models, particularly:</p>

<ul>
  <li><a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/graph_tensor.md"><code class="language-plaintext highlighter-rouge">GraphTensor</code> objects</a>, encapsulating graph structure and features in a TF-compatible format.</li>
  <li><a href="https://github.com/tensorflow/gnn/tree/main/tensorflow_gnn/models">Pre-built GraphUpdate layers</a> implementing some of the message-passing protocols most commonly used in the literature.</li>
  <li>The <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md">orchestrator</a> and related protocols, which can be used to skip most of the technical boilerplate that would otherwise be required for end-to-end training.</li>
</ul>

<h3 id="13-graph-classification"><a id="introduction_graph_classification">1.3 Graph classification</a></h3>

<p>A prototypical example of application of GNNs are <em>graph classification</em> problems, whereupon we are tasked to predict the <em>class</em> of a given graph among a finite set of possible options. In the supervised setting, we have a set of labeled graphs from which we can learn the parameters \(\{\theta_\mathcal{V}, \theta_{\mathcal{E}}\}\) of the GNN by minimizing a categorical crossentropy loss for our predictions. These are obtained from the GNN after applying one final aggregation or <em>pooling</em> operation over all nodes, which again should be orderless to preserve the important property that the output is invariant under relabeling of the nodes.</p>

<p>In this notebook we will undertake the classification of molecules which have been labeled as being either toxic or non-toxic in the <a href="https://www.tensorflow.org/datasets/catalog/cardiotox">CardioTox dataset</a> introduced by:</p>

<p><strong><a id="cardiotox">[1]</a></strong> K. Han, B. Lakshminarayanan and J. Liu, “Reliable Graph Neural Networks for Drug Discovery Under Distributional Shift,” (<a href="https://arxiv.org/abs/2111.12951">arxiv:2111.12951</a>)</p>

<p>We will take the nodes of our graphs to be the atoms in a molecule, with edges representing atomic bonds between them. Thus, our problem is a <strong>binary classification</strong> one (because we have just two classes) on <strong>homogeneous graphs</strong> (because we have only one type of nodes and one type of edges) which are both <strong>node and edge decorated</strong> (because, as will be seen later, the data contains features for atoms and for the bonds between them).</p>

<hr />

<p>The authors of <a href="#cardiotox">[1]</a> introduce various clever architectural choices to improve the performance of their model and reduce the risk of false-negative predictions, which are particularly undesirable in a toxicity-detection problem. For illustration purposes here we will not attempt to reproduce their results in full generality, but instead implement a very simple baseline based on Graph Attention Networks introduced in:</p>

<p><strong><a id="gat">[2]</a></strong> P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò and Y. Bengio “Graph Attention Networks,” (<a href="https://arxiv.org/abs/1710.10903">arxiv:1710.10903</a>)</p>

<p><strong><a id="gat2">[3]</a></strong> S. Brody, U. Alon and E. Yahav, “How Attentive are Graph Attention Networks?,” (<a href="https://arxiv.org/abs/2105.14491">arxiv:2105.14491</a>)</p>

<p>The interested reader can later try different improvements to the code in this notebook, and thanks to TF-GNN these will be easy to introduce once the training pipeline has been established.</p>

<h3 id="14-dependencies-and-imports"><a id="introduction_dependencies">1.4 Dependencies and imports</a></h3>

<p>Since TF-GNN is currently in an early alpha release stage, installation may have a few hiccups. I have found that the following combination works well in the <a href="https://www.kaggle.com/">Kaggle</a> notebook environment:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>

<span class="c1"># install non-Python dependencies
</span><span class="err">!</span><span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="o">-</span><span class="n">y</span> <span class="n">install</span> <span class="n">graphviz</span> <span class="n">graphviz</span><span class="o">-</span><span class="n">dev</span>

<span class="c1"># Upgrade to TensorFlow 2.8
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow</span><span class="o">==</span><span class="mf">2.8</span> <span class="n">tensorflow</span><span class="o">-</span><span class="n">io</span><span class="o">==</span><span class="mf">0.25</span><span class="p">.</span><span class="mi">0</span> <span class="n">tfds</span><span class="o">-</span><span class="n">nightly</span> <span class="n">pygraphviz</span>

<span class="c1"># Install TensorFlow-GNN
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow_gnn</span><span class="o">==</span><span class="mf">0.2</span><span class="p">.</span><span class="mi">0</span>

<span class="c1"># Fix some dependencies
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">httplib2</span><span class="o">==</span><span class="mf">0.20</span><span class="p">.</span><span class="mi">4</span>

<span class="nf">clear_output</span><span class="p">()</span></code></pre></figure>

<p>For <a href="https://colab.research.google.com/drive/1FQPBpdrd17WzX_rcFAMsIaN8EK0rP4Ai?usp=sharing">GoogleColab</a>, you can instead use:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>

<span class="c1"># Install non-Python dependencies
</span><span class="err">!</span><span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="o">-</span><span class="n">y</span> <span class="n">install</span> <span class="n">graphviz</span> <span class="n">graphviz</span><span class="o">-</span><span class="n">dev</span>

<span class="c1"># Install Python dependencies
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">tfds</span><span class="o">-</span><span class="n">nightly</span> <span class="n">pygraphviz</span>

<span class="c1"># Install TensorFlow-GNN
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow_gnn</span><span class="o">==</span><span class="mf">0.2</span><span class="p">.</span><span class="mi">0</span>

<span class="nf">clear_output</span><span class="p">()</span></code></pre></figure>

<p>Either way, once everything is installed we can proceed:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pygraphviz</span> <span class="k">as</span> <span class="n">pgv</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="n">tf</span><span class="p">.</span><span class="nf">get_logger</span><span class="p">().</span><span class="nf">setLevel</span><span class="p">(</span><span class="sh">'</span><span class="s">ERROR</span><span class="sh">'</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">tensorflow_gnn</span> <span class="k">as</span> <span class="n">tfgnn</span>
<span class="kn">import</span> <span class="n">tensorflow_datasets</span> <span class="k">as</span> <span class="n">tfds</span>

<span class="kn">from</span> <span class="n">tensorflow_gnn</span> <span class="kn">import</span> <span class="n">runner</span>
<span class="kn">from</span> <span class="n">tensorflow_gnn.models</span> <span class="kn">import</span> <span class="n">gat_v2</span><span class="p">,</span> <span class="n">gnn_template</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Using TensorFlow v</span><span class="si">{</span><span class="n">tf</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="s"> and TensorFlow-GNN v</span><span class="si">{</span><span class="n">tfgnn</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">GPUs available: </span><span class="si">{</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">list_physical_devices</span><span class="p">(</span><span class="sh">"</span><span class="s">GPU</span><span class="sh">"</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<pre class="output">
Using TensorFlow v2.8.0 and TensorFlow-GNN v0.2.0
GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
</pre>

<div class="section separator"></div>

<h2 id="2-data-preparation"><a id="dataprep">2. Data preparation</a></h2>

<p>Unlike other types of data, there is no standard encoding for graphs. Indeed, depending on the intended application the graph structure can be represented by:</p>
<ul>
  <li>An adjacency matrix \(A_{ij}\) specifying whether the edge \(i\to j\) is present or not in \(\mathcal{E}\).</li>
  <li>An adjacency list \(N_i\) for each node \(i\in\mathcal{V}\), specifying all the nodes \(j\in\mathcal{V}\) for which an edge \((i, j)\in\mathcal{E}\)</li>
  <li>A list of edges \((i,j)\in\mathcal{E}\)</li>
</ul>

<p>To address this issue TF-GNN introduces the <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/graph_tensor.md"><code class="language-plaintext highlighter-rouge">GraphTensor</code> object</a>, which encapsulates both the graph structure and the features of the nodes, edges and the graph itself. These objects follow a <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/schema.md"><em>graph schema</em></a>, which specifies the types of nodes and edges as well as all the features that should be present in the graphs. The first step in any TF-GNN training pipeline should therefore be to convert the input data from whichever format is given into <code class="language-plaintext highlighter-rouge">GraphTensor</code> format. These <code class="language-plaintext highlighter-rouge">GraphTensor</code> objects can then be batched and consumed by our TF-GNN models just like a <code class="language-plaintext highlighter-rouge">tf.Tensor</code>, hugely simplifying our lives in the process.</p>

<p>The goal of this section is to perform the task described above, resulting in <code class="language-plaintext highlighter-rouge">DatasetProvider</code> objects for our data (see <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md#data-reading">here</a> and the following section for a description of the <code class="language-plaintext highlighter-rouge">DatasetProvider</code> protocol). Let us then take a look at the CardioTox dataset, which we can automatically download from TensorFlow Datasets (TF-DS):</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">dataset_splits</span><span class="p">,</span> <span class="n">dataset_info</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">cardiotox</span><span class="sh">'</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">data/tfds</span><span class="sh">'</span><span class="p">,</span> <span class="n">with_info</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="nf">clear_output</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">dataset_info</span><span class="p">.</span><span class="n">description</span><span class="p">)</span></code></pre></figure>

<pre class="output">
Drug Cardiotoxicity dataset [1-2] is a molecule classification task to detect
cardiotoxicity caused by binding hERG target, a protein associated with heart
beat rhythm. The data covers over 9000 molecules with hERG activity.

Note:

1. The data is split into four splits: train, test-iid, test-ood1, test-ood2.

2. Each molecule in the dataset has 2D graph annotations which is designed to
facilitate graph neural network modeling. Nodes are the atoms of the molecule
and edges are the bonds. Each atom is represented as a vector encoding basic
atom information such as atom type. Similar logic applies to bonds.

3. We include Tanimoto fingerprint distance (to training data) for each molecule
in the test sets to facilitate research on distributional shift in graph domain.

For each example, the features include:
  atoms: a 2D tensor with shape (60, 27) storing node features. Molecules with
    less than 60 atoms are padded with zeros. Each atom has 27 atom features.
  pairs: a 3D tensor with shape (60, 60, 12) storing edge features. Each edge
    has 12 edge features.
  atom_mask: a 1D tensor with shape (60, ) storing node masks. 1 indicates the
    corresponding atom is real, othewise a padded one.
  pair_mask: a 2D tensor with shape (60, 60) storing edge masks. 1 indicates the
    corresponding edge is real, othewise a padded one.
  active: a one-hot vector indicating if the molecule is toxic or not. [0, 1]
    indicates it's toxic, otherwise [1, 0] non-toxic.


## References
[1]: V. B. Siramshetty et al. Critical Assessment of Artificial Intelligence
Methods for Prediction of hERG Channel Inhibition in the Big Data Era.
    JCIM, 2020. https://pubs.acs.org/doi/10.1021/acs.jcim.0c00884

[2]: K. Han et al. Reliable Graph Neural Networks for Drug Discovery Under
Distributional Shift.
    NeurIPS DistShift Workshop 2021. https://arxiv.org/abs/2111.12951
</pre>

<p>As mentioned before, we will have only one set of nodes, which we call <code class="language-plaintext highlighter-rouge">'atom'</code>, and one set of edges, which we call <code class="language-plaintext highlighter-rouge">'bond'</code>. Of course, all edges have <code class="language-plaintext highlighter-rouge">'atom'</code>-type nodes at both endpoints. Both nodes and edges have a single feature vector, the former being a 27-dimensional <code class="language-plaintext highlighter-rouge">'atom_features'</code> vector, and the latter being a 12-dimensional <code class="language-plaintext highlighter-rouge">'bond_features'</code> vector. Moreover, the graphs themselves have global features giving their <em>context</em>, in this case a toxicity class <code class="language-plaintext highlighter-rouge">'toxicity'</code>, which is in fact the label we want to predict, and a molecule id <code class="language-plaintext highlighter-rouge">'molecule_id'</code> which we will mostly ignore.</p>

<p>All of the above can be encoded in the following graph schema specifying the structure and contents of our graphs:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_schema_pbtxt</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
node_sets {
  key: </span><span class="sh">"</span><span class="s">atom</span><span class="sh">"</span><span class="s">
  value {
    description: </span><span class="sh">"</span><span class="s">An atom in the molecule.</span><span class="sh">"</span><span class="s">

    features {
      key: </span><span class="sh">"</span><span class="s">atom_features</span><span class="sh">"</span><span class="s">
      value: {
        description: </span><span class="sh">"</span><span class="s">[DATA] The features of the atom.</span><span class="sh">"</span><span class="s">
        dtype: DT_FLOAT
        shape { dim { size: 27 } }
      }
    }
  }
}

edge_sets {
  key: </span><span class="sh">"</span><span class="s">bond</span><span class="sh">"</span><span class="s">
  value {
    description: </span><span class="sh">"</span><span class="s">A bond between two atoms in the molecule.</span><span class="sh">"</span><span class="s">
    source: </span><span class="sh">"</span><span class="s">atom</span><span class="sh">"</span><span class="s">
    target: </span><span class="sh">"</span><span class="s">atom</span><span class="sh">"</span><span class="s">

    features {
      key: </span><span class="sh">"</span><span class="s">bond_features</span><span class="sh">"</span><span class="s">
      value: {
        description: </span><span class="sh">"</span><span class="s">[DATA] The features of the bond.</span><span class="sh">"</span><span class="s">
        dtype: DT_FLOAT
        shape { dim { size: 12 } }
      }
    }
  }
}

context {
  features {
    key: </span><span class="sh">"</span><span class="s">toxicity</span><span class="sh">"</span><span class="s">
    value: {
      description: </span><span class="sh">"</span><span class="s">[LABEL] The toxicity class of the molecule (0 -&gt; non-toxic; 1 -&gt; toxic).</span><span class="sh">"</span><span class="s">
      dtype: DT_INT64
    }
  }
  
  features {
    key: </span><span class="sh">"</span><span class="s">molecule_id</span><span class="sh">"</span><span class="s">
    value: {
      description: </span><span class="sh">"</span><span class="s">[LABEL] The id of the molecule.</span><span class="sh">"</span><span class="s">
      dtype: DT_STRING
    }
  }
}
</span><span class="sh">"""</span></code></pre></figure>

<p>This schema is a textual protobuf, which we can parse to obtain a <code class="language-plaintext highlighter-rouge">GraphTensorSpec</code> (think <code class="language-plaintext highlighter-rouge">TensorSpec</code> for <code class="language-plaintext highlighter-rouge">GraphTensor</code> objects):</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_schema</span> <span class="o">=</span> <span class="n">tfgnn</span><span class="p">.</span><span class="nf">parse_schema</span><span class="p">(</span><span class="n">graph_schema_pbtxt</span><span class="p">)</span>
<span class="n">graph_spec</span> <span class="o">=</span> <span class="n">tfgnn</span><span class="p">.</span><span class="nf">create_graph_spec_from_schema_pb</span><span class="p">(</span><span class="n">graph_schema</span><span class="p">)</span></code></pre></figure>

<p>We should then convert the input dataset into <code class="language-plaintext highlighter-rouge">GraphTensor</code> objects complying to <code class="language-plaintext highlighter-rouge">graph_spec</code>, which we will do with the following helper function:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">make_graph_tensor</span><span class="p">(</span><span class="n">datapoint</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Convert a datapoint from the TF-DS CardioTox dataset into a `GraphTensor`.
    </span><span class="sh">"""</span>
    <span class="c1"># atom_mask is non-zero only for real atoms
</span>    <span class="c1"># [ V, ]
</span>    <span class="n">atom_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">datapoint</span><span class="p">[</span><span class="sh">'</span><span class="s">atom_mask</span><span class="sh">'</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># only keep features of real atoms
</span>    <span class="c1"># [ V, 27 ]
</span>    <span class="n">atom_features</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">datapoint</span><span class="p">[</span><span class="sh">'</span><span class="s">atoms</span><span class="sh">'</span><span class="p">],</span> <span class="n">atom_indices</span><span class="p">)</span>
    
    <span class="c1"># restrict the bond mask to real atoms
</span>    <span class="c1"># [ V, V ]
</span>    <span class="n">pair_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">datapoint</span><span class="p">[</span><span class="sh">'</span><span class="s">pair_mask</span><span class="sh">'</span><span class="p">],</span> <span class="n">atom_indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">atom_indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># restrict the bond features to real atoms
</span>    <span class="c1"># [ V, V, 12 ]
</span>    <span class="n">pairs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">datapoint</span><span class="p">[</span><span class="sh">'</span><span class="s">pairs</span><span class="sh">'</span><span class="p">],</span> <span class="n">atom_indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">atom_indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># pair_mask is non-zero only for real bonds
</span>    <span class="c1"># [ E, 2 ]
</span>    <span class="n">bond_indices</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">pair_mask</span><span class="p">)</span>
    
    <span class="c1"># only keep features of real bonds
</span>    <span class="c1"># [ E, 12 ]
</span>    <span class="n">bond_features</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gather_nd</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">bond_indices</span><span class="p">)</span>
    
    <span class="c1"># separate sources and targets for each bond
</span>    <span class="c1"># [ E, ]
</span>    <span class="n">sources</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">unstack</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">bond_indices</span><span class="p">))</span>

    <span class="c1"># active is [1, 0] for non-toxic molecules, [0, 1] for toxic molecules
</span>    <span class="c1"># [ ]
</span>    <span class="n">toxicity</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">datapoint</span><span class="p">[</span><span class="sh">'</span><span class="s">active</span><span class="sh">'</span><span class="p">])</span>
    
    <span class="c1"># the molecule_id is included for reference
</span>    <span class="c1"># [ ]
</span>    <span class="n">molecule_id</span> <span class="o">=</span> <span class="n">datapoint</span><span class="p">[</span><span class="sh">'</span><span class="s">molecule_id</span><span class="sh">'</span><span class="p">]</span>

    <span class="c1"># create a GraphTensor from all of the above
</span>    <span class="n">atom</span> <span class="o">=</span> <span class="n">tfgnn</span><span class="p">.</span><span class="n">NodeSet</span><span class="p">.</span><span class="nf">from_fields</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">atom_features</span><span class="sh">'</span><span class="p">:</span> <span class="n">atom_features</span><span class="p">},</span>
                                     <span class="n">sizes</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">atom_indices</span><span class="p">))</span>
    
    <span class="n">atom_adjacency</span> <span class="o">=</span> <span class="n">tfgnn</span><span class="p">.</span><span class="n">Adjacency</span><span class="p">.</span><span class="nf">from_indices</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="p">(</span><span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">sources</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)),</span>
                                                  <span class="n">target</span><span class="o">=</span><span class="p">(</span><span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)))</span>
    
    <span class="n">bond</span> <span class="o">=</span> <span class="n">tfgnn</span><span class="p">.</span><span class="n">EdgeSet</span><span class="p">.</span><span class="nf">from_fields</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">bond_features</span><span class="sh">'</span><span class="p">:</span> <span class="n">bond_features</span><span class="p">},</span>
                                     <span class="n">sizes</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">sources</span><span class="p">),</span>
                                     <span class="n">adjacency</span><span class="o">=</span><span class="n">atom_adjacency</span><span class="p">)</span>
    
    <span class="n">context</span> <span class="o">=</span> <span class="n">tfgnn</span><span class="p">.</span><span class="n">Context</span><span class="p">.</span><span class="nf">from_fields</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">toxicity</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="n">toxicity</span><span class="p">],</span> <span class="sh">'</span><span class="s">molecule_id</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="n">molecule_id</span><span class="p">]})</span>
    
    <span class="k">return</span> <span class="n">tfgnn</span><span class="p">.</span><span class="n">GraphTensor</span><span class="p">.</span><span class="nf">from_pieces</span><span class="p">(</span><span class="n">node_sets</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">:</span> <span class="n">atom</span><span class="p">},</span> <span class="n">edge_sets</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">:</span> <span class="n">bond</span><span class="p">},</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span></code></pre></figure>

<p>We can now map this function over the datasets to have them stream <code class="language-plaintext highlighter-rouge">GraphTensor</code> objects:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dataset_splits</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">(</span><span class="n">make_graph_tensor</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))</span>
<span class="n">graph_tensor</span></code></pre></figure>

<pre class="output">
GraphTensor(
  context=Context(features={'toxicity': &lt;tf.Tensor: shape=(1,), dtype=tf.int64&gt;, 'molecule_id': &lt;tf.Tensor: shape=(1,), dtype=tf.string&gt;}, sizes=[1], shape=(), indices_dtype=tf.int32),
  node_set_names=['atom'],
  edge_set_names=['bond'])
</pre>

<p>And check that the <code class="language-plaintext highlighter-rouge">GraphTensor</code> thus produced are compatible with the <code class="language-plaintext highlighter-rouge">GraphTensorSpec</code> we defined before:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_spec</span><span class="p">.</span><span class="nf">is_compatible_with</span><span class="p">(</span><span class="n">graph_tensor</span><span class="p">)</span></code></pre></figure>

<pre class="output">
True
</pre>

<p>However, to avoid processing the data multiple times, which would slow down all of our input pipeline, it is convenient to first dump all the data into TFRecord files. Later on we can easily load these instead of the original TF-DS datasets over which we mapped the <code class="language-plaintext highlighter-rouge">make_graph_tensor</code> function.</p>

<p><strong>NOTE:</strong> The <code class="language-plaintext highlighter-rouge">create_tfrecords</code> method below works nicely, is rather general and could be immediately reused for other small-scale applications. For large-scale datasets, however, alternative approaches using <code class="language-plaintext highlighter-rouge">tf.data.Dataset.cache</code> or <code class="language-plaintext highlighter-rouge">tf.data.Dataset.snapshot</code> would be preferable, as they would allow for more optimizations such as <em>e.g.</em> sharding.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">create_tfrecords</span><span class="p">(</span><span class="n">dataset_splits</span><span class="p">,</span> <span class="n">dataset_info</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Dump all splits of the given dataset to TFRecord files.
    </span><span class="sh">"""</span>
    <span class="k">for</span> <span class="n">split_name</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="n">dataset_splits</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">data/</span><span class="si">{</span><span class="n">dataset_info</span><span class="p">.</span><span class="n">name</span><span class="si">}</span><span class="s">-</span><span class="si">{</span><span class="n">split_name</span><span class="si">}</span><span class="s">.tfrecord</span><span class="sh">'</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">creating </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s">...</span><span class="sh">'</span><span class="p">)</span>
        
        <span class="c1"># convert all datapoints to GraphTensor
</span>        <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="n">make_graph_tensor</span><span class="p">,</span> <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
        
        <span class="c1"># serialize to TFRecord files
</span>        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="nc">TFRecordWriter</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">graph_tensor</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="n">dataset_info</span><span class="p">.</span><span class="n">splits</span><span class="p">[</span><span class="n">split_name</span><span class="p">].</span><span class="n">num_examples</span><span class="p">):</span>
                <span class="n">example</span> <span class="o">=</span> <span class="n">tfgnn</span><span class="p">.</span><span class="nf">write_example</span><span class="p">(</span><span class="n">graph_tensor</span><span class="p">)</span>
                <span class="n">writer</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">example</span><span class="p">.</span><span class="nc">SerializeToString</span><span class="p">())</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nf">create_tfrecords</span><span class="p">(</span><span class="n">dataset_splits</span><span class="p">,</span> <span class="n">dataset_info</span><span class="p">)</span></code></pre></figure>

<pre class="output">
creating data/cardiotox-train.tfrecord...
100%|██████████| 6523/6523 [00:46&lt;00:00, 140.23it/s]
creating data/cardiotox-validation.tfrecord...
100%|██████████| 1631/1631 [00:11&lt;00:00, 138.38it/s]
creating data/cardiotox-test.tfrecord...
100%|██████████| 839/839 [00:05&lt;00:00, 142.62it/s]
creating data/cardiotox-test2.tfrecord...
100%|██████████| 177/177 [00:01&lt;00:00, 140.72it/s]
</pre>

<p>Finally, we can use the <code class="language-plaintext highlighter-rouge">TFRecordDatasetProvider</code> class to create <code class="language-plaintext highlighter-rouge">DatasetProvider</code>-compliant objects that read these TFRecord files and provide <code class="language-plaintext highlighter-rouge">tf.data.Dataset</code> objects for us to use through their <code class="language-plaintext highlighter-rouge">get_dataset</code> method:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">train_dataset_provider</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="nc">TFRecordDatasetProvider</span><span class="p">(</span><span class="n">file_pattern</span><span class="o">=</span><span class="sh">'</span><span class="s">data/cardiotox-train.tfrecord</span><span class="sh">'</span><span class="p">)</span>
<span class="n">valid_dataset_provider</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="nc">TFRecordDatasetProvider</span><span class="p">(</span><span class="n">file_pattern</span><span class="o">=</span><span class="sh">'</span><span class="s">data/cardiotox-validation.tfrecord</span><span class="sh">'</span><span class="p">)</span>
<span class="n">test1_dataset_provider</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="nc">TFRecordDatasetProvider</span><span class="p">(</span><span class="n">file_pattern</span><span class="o">=</span><span class="sh">'</span><span class="s">data/cardiotox-test.tfrecord</span><span class="sh">'</span><span class="p">)</span>
<span class="n">test2_dataset_provider</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="nc">TFRecordDatasetProvider</span><span class="p">(</span><span class="n">file_pattern</span><span class="o">=</span><span class="sh">'</span><span class="s">data/cardiotox-test2.tfrecord</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<h3 id="21-the-datasetprovider-protocol"><a id="dataprep_datasetprovider">2.1 The DatasetProvider protocol</a></h3>

<p>Each of the <code class="language-plaintext highlighter-rouge">DatasetProvider</code> defined above conventionally produces a dataset of serialized <code class="language-plaintext highlighter-rouge">GraphTensor</code> objects, which we need to parse before we can inspect. This is mentioned here for reference purposes only: the <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md#orchestration">orchestrator</a> will transparently deal with this during actual training.</p>

<p>To obtain the dataset we need to provide an input context:<a id="input_context"></a></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset_provider</span><span class="p">.</span><span class="nf">get_dataset</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">distribute</span><span class="p">.</span><span class="nc">InputContext</span><span class="p">())</span></code></pre></figure>

<p>We then map <code class="language-plaintext highlighter-rouge">tfgnn.parse_single_example</code> over this dataset, specifying the appropriate <code class="language-plaintext highlighter-rouge">GraphTensorSpec</code> for our graphs:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">serialized</span><span class="p">:</span> <span class="n">tfgnn</span><span class="p">.</span><span class="nf">parse_single_example</span><span class="p">(</span><span class="n">serialized</span><span class="o">=</span><span class="n">serialized</span><span class="p">,</span> <span class="n">spec</span><span class="o">=</span><span class="n">graph_spec</span><span class="p">))</span></code></pre></figure>

<p>And we can then stream <code class="language-plaintext highlighter-rouge">GraphTensor</code> objects as before</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))</span>
<span class="n">graph_tensor</span></code></pre></figure>

<pre class="output">
GraphTensor(
  context=Context(features={'toxicity': &lt;tf.Tensor: shape=(1,), dtype=tf.int64&gt;, 'molecule_id': &lt;tf.Tensor: shape=(1,), dtype=tf.string&gt;}, sizes=[1], shape=(), indices_dtype=tf.int32),
  node_set_names=['atom'],
  edge_set_names=['bond'])
</pre>

<h3 id="22-data-inspection"><a id="dataprep_inspection">2.2 Data inspection</a></h3>

<p>The node and edge features are not particularly illustrative, but we can nevertheless access them directly if necessary. First, note that this particular molecule has the following number \(V = \left\vert\mathcal{V}\right\vert\) of atoms:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor</span><span class="p">.</span><span class="n">node_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">].</span><span class="n">sizes</span></code></pre></figure>

<pre class="output">
&lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([33], dtype=int32)&gt;
</pre>

<p>Their features are collected in a tensor of shape <code class="language-plaintext highlighter-rouge">(V, 27)</code>, which we can access like so:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor</span><span class="p">.</span><span class="n">node_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">atom_features</span><span class="sh">'</span><span class="p">]</span></code></pre></figure>

<pre class="output">
&lt;tf.Tensor: shape=(33, 27), dtype=float32, numpy=
array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.],
       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 1.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 1.],
       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.],
       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.]], dtype=float32)&gt;
</pre>

<p>Similarly, the number \(E = \left\vert\mathcal{E}\right\vert\) of bonds in the molecule is:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor</span><span class="p">.</span><span class="n">edge_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">].</span><span class="n">sizes</span></code></pre></figure>

<pre class="output">
&lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([68], dtype=int32)&gt;
</pre>

<p>Their features are collected in a tensor of shape <code class="language-plaintext highlighter-rouge">(E, 12)</code>, which we can access like so:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor</span><span class="p">.</span><span class="n">edge_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">bond_features</span><span class="sh">'</span><span class="p">]</span></code></pre></figure>

<pre class="output">
&lt;tf.Tensor: shape=(68, 12), dtype=float32, numpy=
array([[1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
       [0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],
       [0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.]], dtype=float32)&gt;
</pre>

<p>The ids of the edge endpoints are then stored in a couple of tensors of shape <code class="language-plaintext highlighter-rouge">(E,)</code></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor</span><span class="p">.</span><span class="n">edge_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">].</span><span class="n">adjacency</span><span class="p">.</span><span class="n">source</span></code></pre></figure>

<pre class="output">
&lt;tf.Tensor: shape=(68,), dtype=int32, numpy=
array([ 0,  1,  1,  1,  2,  2,  2,  3,  3,  4,  4,  5,  5,  5,  6,  7,  7,
        8,  8,  9,  9, 10, 10, 10, 11, 12, 12, 13, 13, 14, 14, 15, 15, 15,
       16, 17, 17, 18, 18, 19, 19, 20, 20, 20, 21, 22, 23, 23, 23, 24, 25,
       25, 25, 26, 26, 27, 27, 28, 28, 29, 29, 30, 30, 30, 31, 31, 31, 32],
      dtype=int32)&gt;
</pre>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor</span><span class="p">.</span><span class="n">edge_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">].</span><span class="n">adjacency</span><span class="p">.</span><span class="n">target</span></code></pre></figure>

<pre class="output">
&lt;tf.Tensor: shape=(68,), dtype=int32, numpy=
array([ 1,  0,  2, 31,  1,  3, 23,  2,  4,  3,  5,  4,  6,  7,  5,  5,  8,
        7,  9,  8, 10,  9, 11, 12, 10, 10, 13, 12, 14, 13, 15, 14, 16, 17,
       15, 15, 18, 17, 19, 18, 20, 19, 21, 22, 20, 20,  2, 24, 25, 23, 23,
       26, 30, 25, 27, 26, 28, 27, 29, 28, 30, 25, 29, 31,  1, 30, 32, 31],
      dtype=int32)&gt;
</pre>

<p>Finally, global information about the graph is provided by its context</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor</span><span class="p">.</span><span class="n">context</span><span class="p">[</span><span class="sh">'</span><span class="s">toxicity</span><span class="sh">'</span><span class="p">]</span></code></pre></figure>

<pre class="output">
&lt;tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])&gt;
</pre>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor</span><span class="p">.</span><span class="n">context</span><span class="p">[</span><span class="sh">'</span><span class="s">molecule_id</span><span class="sh">'</span><span class="p">]</span></code></pre></figure>

<pre class="output">
&lt;tf.Tensor: shape=(1,), dtype=string, numpy=
array([b'CC1=C(C/C=C(\\C)CCC[C@H](C)CCC[C@H](C)CCCC(C)C)C(=O)c2ccccc2C1=O'],
      dtype=object)&gt;
</pre>

<p>With all of this we can write the following helper function to visualize the graphs:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">draw_molecule</span><span class="p">(</span><span class="n">graph_tensor</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Plot the `GraphTensor` representation of a molecule.
    </span><span class="sh">"""</span>
    <span class="p">(</span><span class="n">molecule_id</span><span class="p">,)</span> <span class="o">=</span> <span class="n">graph_tensor</span><span class="p">.</span><span class="n">context</span><span class="p">[</span><span class="sh">'</span><span class="s">molecule_id</span><span class="sh">'</span><span class="p">].</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="p">(</span><span class="n">toxicity</span><span class="p">,)</span> <span class="o">=</span> <span class="n">graph_tensor</span><span class="p">.</span><span class="n">context</span><span class="p">[</span><span class="sh">'</span><span class="s">toxicity</span><span class="sh">'</span><span class="p">].</span><span class="nf">numpy</span><span class="p">()</span>

    <span class="n">sources</span> <span class="o">=</span> <span class="n">graph_tensor</span><span class="p">.</span><span class="n">edge_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">].</span><span class="n">adjacency</span><span class="p">.</span><span class="n">source</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">graph_tensor</span><span class="p">.</span><span class="n">edge_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">].</span><span class="n">adjacency</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

    <span class="n">pgvGraph</span> <span class="o">=</span> <span class="n">pgv</span><span class="p">.</span><span class="nc">AGraph</span><span class="p">()</span>
    <span class="n">pgvGraph</span><span class="p">.</span><span class="n">graph_attr</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">toxicity = </span><span class="si">{</span><span class="n">toxicity</span><span class="si">}</span><span class="se">\n\n</span><span class="s">molecule_id = </span><span class="si">{</span><span class="n">molecule_id</span><span class="p">.</span><span class="nf">decode</span><span class="p">()</span><span class="si">}</span><span class="sh">'</span>

    <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">sources</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="n">pgvGraph</span><span class="p">.</span><span class="nf">add_edge</span><span class="p">(</span><span class="n">edge</span><span class="p">)</span>

    <span class="k">return</span> <span class="nc">Image</span><span class="p">(</span><span class="n">pgvGraph</span><span class="p">.</span><span class="nf">draw</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="sh">'</span><span class="s">png</span><span class="sh">'</span><span class="p">,</span> <span class="n">prog</span><span class="o">=</span><span class="sh">'</span><span class="s">dot</span><span class="sh">'</span><span class="p">))</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nf">draw_molecule</span><span class="p">(</span><span class="n">graph_tensor</span><span class="p">)</span></code></pre></figure>

<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog/tfgnn-intro/molecule-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog/tfgnn-intro/molecule-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog/tfgnn-intro/molecule-1400.webp" />
    <!-- Fallback to the original file -->
    <!-- style="max-width:100%;max-height:100%" -->
    <img src="/assets/img/blog/tfgnn-intro/molecule.png" />

  </picture>

</figure>

<h3 id="23-graphtensor-batching"><a id="dataprep_batching">2.3 GraphTensor batching</a></h3>

<p><code class="language-plaintext highlighter-rouge">GraphTensor</code> datasets can be batched as usual, resulting in new datasets that produce higher-rank <code class="language-plaintext highlighter-rouge">GraphTensor</code> objects:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">batched_train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor_batch</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">batched_train_dataset</span><span class="p">))</span>
<span class="n">graph_tensor_batch</span><span class="p">.</span><span class="n">rank</span></code></pre></figure>

<pre class="output">
1
</pre>

<p>The resulting <code class="language-plaintext highlighter-rouge">GraphTensor</code> now contains features in the form of <code class="language-plaintext highlighter-rouge">tf.RaggedTensor</code>, since different graphs can have different numbers of nodes and edges:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor_batch</span><span class="p">.</span><span class="n">node_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">atom_features</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span></code></pre></figure>

<pre class="output">
TensorShape([64, None, 27])
</pre>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_tensor_batch</span><span class="p">.</span><span class="n">edge_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">bond_features</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span></code></pre></figure>

<pre class="output">
TensorShape([64, None, 12])
</pre>

<p>where the shapes now correspond to <code class="language-plaintext highlighter-rouge">(batch_size, V, 27)</code> and <code class="language-plaintext highlighter-rouge">(batch_size, E, 12)</code>.</p>

<p>However, all layers in TF-GNN expect scalar graphs as their inputs, so before actually using a batch of graphs we should always “merge” the different graphs in the batch into a single graph with multiple disconnected components (of which TF-GNN automatically keeps track):</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">scalar_graph_tensor</span> <span class="o">=</span> <span class="n">graph_tensor_batch</span><span class="p">.</span><span class="nf">merge_batch_to_components</span><span class="p">()</span>
<span class="n">scalar_graph_tensor</span><span class="p">.</span><span class="n">rank</span></code></pre></figure>

<pre class="output">
0
</pre>

<p>Now the atom features again have shape <code class="language-plaintext highlighter-rouge">(V', 27)</code>, where \(V' = \sum_{k=1}^{\rm batch\_size} V_k\)</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">scalar_graph_tensor</span><span class="p">.</span><span class="n">node_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">atom_features</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span></code></pre></figure>

<pre class="output">
TensorShape([1562, 27])
</pre>

<p>And the bond fetures have shape \((E', 12)\) with \(E' = \sum_{k=1}^{\rm batch\_size} E_k\)</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">scalar_graph_tensor</span><span class="p">.</span><span class="n">edge_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">bond_features</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span></code></pre></figure>

<pre class="output">
TensorShape([3370, 12])
</pre>

<p>We should note, however, that once more the <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md#orchestration">orchestrator</a> will transparently take care of batching and merging components for us, so that we needn’t worry about this as long as we don’t customize the training routines.</p>

<div class="section separator"></div>

<h2 id="3-vanilla-mpnn-models"><a id="mpnn">3. Vanilla MPNN models</a></h2>

<p>A <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/gnn_modeling.md">common architecture for GNNs</a> consists of an initial layer which preprocesses the graph features, typically producing hidden states for nodes and/or edges, which is followed by one or more layers of message-passing working as described in the <a href="#introduction">Introduction</a>. The goal of this section is to define a <code class="language-plaintext highlighter-rouge">vanilla_mpnn_model</code> function which can be used to create such simple GNNs from:</p>

<ul>
  <li>an initial layer performing the pre-processing</li>
  <li>a layer stacking multiple message-passing layers</li>
</ul>

<h3 id="31-initial-graph-embedding"><a id="mpnn_embedding">3.1 Initial graph embedding</a></h3>

<p>For the first of these tasks we will use a <code class="language-plaintext highlighter-rouge">tfgnn.keras.layers.MapFeatures</code> layer to create hidden state vectors for atoms and bonds from their respective features, by passing these through a dense layer. The resulting hidden states will have dimension <code class="language-plaintext highlighter-rouge">hidden_size</code>, corresponding in the notation of the <a href="#introduction_graphs">Introduction</a> to \(d_\mathcal{V}\) and \(d_\mathcal{E}\).</p>

<p>The following helper function will create an initial <code class="language-plaintext highlighter-rouge">MapFeatures</code> layer for the given hyperparameters:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">hidden_size</code>: the hidden dimensions \(d_\mathcal{V}\) and \(d_\mathcal{E}\)</li>
  <li><code class="language-plaintext highlighter-rouge">activation</code>: the activation for the dense layers</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_initial_map_features</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Initial pre-processing layer for a GNN (use as a class constructor).
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">node_sets_fn</span><span class="p">(</span><span class="n">node_set</span><span class="p">,</span> <span class="n">node_set_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">node_set_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)(</span><span class="n">node_set</span><span class="p">[</span><span class="sh">'</span><span class="s">atom_features</span><span class="sh">'</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">edge_sets_fn</span><span class="p">(</span><span class="n">edge_set</span><span class="p">,</span> <span class="n">edge_set_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">edge_set_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)(</span><span class="n">edge_set</span><span class="p">[</span><span class="sh">'</span><span class="s">bond_features</span><span class="sh">'</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">tfgnn</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">MapFeatures</span><span class="p">(</span><span class="n">node_sets_fn</span><span class="o">=</span><span class="n">node_sets_fn</span><span class="p">,</span>
                                          <span class="n">edge_sets_fn</span><span class="o">=</span><span class="n">edge_sets_fn</span><span class="p">,</span>
                                          <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">graph_embedding</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<p>We can check that the resulting layer replaces the <code class="language-plaintext highlighter-rouge">'atom_features'</code> and <code class="language-plaintext highlighter-rouge">'bond_features'</code> with hidden states of the specified dimensions</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">graph_embedding</span> <span class="o">=</span> <span class="nf">get_initial_map_features</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">embedded_graph</span> <span class="o">=</span> <span class="nf">graph_embedding</span><span class="p">(</span><span class="n">scalar_graph_tensor</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">embedded_graph</span><span class="p">.</span><span class="n">node_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">].</span><span class="n">features</span></code></pre></figure>

<pre class="output">
{'hidden_state': &lt;tf.Tensor: shape=(1562, 128), dtype=float32, numpy=
array([[0.15272579, 0.        , 0.        , ..., 0.        , 0.        ,
        0.00244589],
       [0.20299977, 0.15906705, 0.        , ..., 0.2211346 , 0.        ,
        0.23549727],
       [0.20299977, 0.15906705, 0.        , ..., 0.2211346 , 0.        ,
        0.23549727],
       ...,
       [0.3452986 , 0.        , 0.01703222, ..., 0.        , 0.        ,
        0.        ],
       [0.23036027, 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.595324  , 0.        , 0.3375612 , ..., 0.23380664, 0.        ,
        0.11104017]], dtype=float32)&gt;}
</pre>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">embedded_graph</span><span class="p">.</span><span class="n">edge_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">].</span><span class="n">features</span></code></pre></figure>

<pre class="output">
{'hidden_state': &lt;tf.Tensor: shape=(3370, 128), dtype=float32, numpy=
array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.19551635, 0.05461648, ..., 0.        , 0.06973472,
        0.        ],
       ...,
       [0.01244138, 0.        , 0.        , ..., 0.        , 0.02111641,
        0.        ],
       [0.01244138, 0.        , 0.        , ..., 0.        , 0.02111641,
        0.        ],
       [0.01244138, 0.        , 0.        , ..., 0.        , 0.02111641,
        0.        ]], dtype=float32)&gt;}
</pre>

<p>Note that both the atom and bond features are now named <code class="language-plaintext highlighter-rouge">'hidden_state'</code>; we could of course have chosen a different name, but leaving the default <code class="language-plaintext highlighter-rouge">tfgnn.HIDDEN_STATE</code> will save us from having to specify feature names in what follows.</p>

<h3 id="32-stacking-message-passing-layers"><a id="mpnn_stack">3.2 Stacking message-passing layers</a></h3>

<p>To illustrate how to build a stack of message-passing layers, we will use the pre-built <a href="#gat">Graph Attention (GAT) [2]</a> layers provided in the <a href="https://github.com/tensorflow/gnn/tree/main/tensorflow_gnn/models/gat_v2"><code class="language-plaintext highlighter-rouge">models.gat_v2</code> module</a>. We then define a message-passing neural network (MPNN) layer successively applying these layers with hyperparameters:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">hidden_size</code>: the hidden dimensions \(d_\mathcal{V}\) and \(d_\mathcal{E}\)</li>
  <li><code class="language-plaintext highlighter-rouge">hops</code>: the number of layers in the stack</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">MPNN</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    A basic stack of message-passing Graph Attention layers.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hops</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">gat_mpnn</span><span class="sh">'</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hops</span> <span class="o">=</span> <span class="n">hops</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">mp_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nf">_mp_factory</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">message_passing_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">hops</span><span class="p">)]</span>
    
    <span class="k">def</span> <span class="nf">_mp_factory</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">gat_v2</span><span class="p">.</span><span class="nc">GATv2GraphUpdate</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                       <span class="n">per_head_channels</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span>
                                       <span class="n">edge_set_name</span><span class="o">=</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">,</span>
                                       <span class="n">sender_edge_feature</span><span class="o">=</span><span class="n">tfgnn</span><span class="p">.</span><span class="n">HIDDEN_STATE</span><span class="p">,</span>
                                       <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nf">super</span><span class="p">().</span><span class="nf">get_config</span><span class="p">()</span>
        <span class="n">config</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span>
            <span class="sh">'</span><span class="s">hidden_size</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">hops</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">hops</span>
        <span class="p">})</span>
        <span class="k">return</span> <span class="n">config</span>
        
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">graph_tensor</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">mp_layers</span><span class="p">:</span>
            <span class="n">graph_tensor</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">graph_tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">graph_tensor</span></code></pre></figure>

<p>We can now check that this layer processes the embedded graphs that come out from the initial feature map:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">mpnn</span> <span class="o">=</span> <span class="nc">MPNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">hops</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">hidden_graph</span> <span class="o">=</span> <span class="nf">mpnn</span><span class="p">(</span><span class="n">embedded_graph</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">hidden_graph</span><span class="p">.</span><span class="n">node_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">].</span><span class="n">features</span></code></pre></figure>

<pre class="output">
{'hidden_state': &lt;tf.Tensor: shape=(1562, 128), dtype=float32, numpy=
array([[0.        , 0.        , 0.4671823 , ..., 0.18130356, 0.        ,
        0.        ],
       [0.        , 0.        , 0.4360276 , ..., 0.20995092, 0.        ,
        0.        ],
       [0.        , 0.        , 0.4278612 , ..., 0.20583078, 0.        ,
        0.        ],
       ...,
       [0.        , 0.00174278, 0.51690316, ..., 0.20055819, 0.        ,
        0.        ],
       [0.        , 0.01096402, 0.54201955, ..., 0.20254537, 0.        ,
        0.        ],
       [0.        , 0.0008509 , 0.5069808 , ..., 0.19695306, 0.        ,
        0.        ]], dtype=float32)&gt;}
</pre>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">hidden_graph</span><span class="p">.</span><span class="n">edge_sets</span><span class="p">[</span><span class="sh">'</span><span class="s">bond</span><span class="sh">'</span><span class="p">].</span><span class="n">features</span></code></pre></figure>

<pre class="output">
{'hidden_state': &lt;tf.Tensor: shape=(3370, 128), dtype=float32, numpy=
array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,
        0.        ],
       [0.        , 0.19551635, 0.05461648, ..., 0.        , 0.06973472,
        0.        ],
       ...,
       [0.01244138, 0.        , 0.        , ..., 0.        , 0.02111641,
        0.        ],
       [0.01244138, 0.        , 0.        , ..., 0.        , 0.02111641,
        0.        ],
       [0.01244138, 0.        , 0.        , ..., 0.        , 0.02111641,
        0.        ]], dtype=float32)&gt;}
</pre>

<h3 id="33-model-construction"><a id="mpnn_construction">3.3 Model construction</a></h3>

<p>We are now ready to combine both ingredients into a <code class="language-plaintext highlighter-rouge">tf.keras.Model</code> that takes a <code class="language-plaintext highlighter-rouge">GraphTensor</code> representing a molecule as input, and produces as its output another <code class="language-plaintext highlighter-rouge">GraphTensor</code> with hidden states for all the atoms. We use Keras’ functional API to define a <code class="language-plaintext highlighter-rouge">vanilla_mpnn_model</code> helper function returning the desired <code class="language-plaintext highlighter-rouge">tf.keras.Model</code>:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">vanilla_mpnn_model</span><span class="p">(</span><span class="n">graph_tensor_spec</span><span class="p">,</span> <span class="n">init_states_fn</span><span class="p">,</span> <span class="n">pass_messages_fn</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Chain an initialization layer and a message-passing stack to produce a `tf.keras.Model`.
    </span><span class="sh">"""</span>
    <span class="n">graph_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">type_spec</span><span class="o">=</span><span class="n">graph_tensor_spec</span><span class="p">)</span>
    <span class="n">embedded_graph</span> <span class="o">=</span> <span class="nf">init_states_fn</span><span class="p">(</span><span class="n">graph_tensor</span><span class="p">)</span>
    <span class="n">hidden_graph</span> <span class="o">=</span> <span class="nf">pass_messages_fn</span><span class="p">(</span><span class="n">embedded_graph</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">graph_tensor</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">hidden_graph</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="nf">vanilla_mpnn_model</span><span class="p">(</span><span class="n">graph_tensor_spec</span><span class="o">=</span><span class="n">graph_spec</span><span class="p">,</span>
                           <span class="n">init_states_fn</span><span class="o">=</span><span class="n">graph_embedding</span><span class="p">,</span>
                           <span class="n">pass_messages_fn</span><span class="o">=</span><span class="n">mpnn</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span></code></pre></figure>

<pre class="output">
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [()]                      0         
                                                                 
 graph_embedding (MapFeature  ()                       5248      
 s)                                                              
                                                                 
 gat_mpnn (MPNN)             ()                        396288    
                                                                 
=================================================================
Total params: 401,536
Trainable params: 401,536
Non-trainable params: 0
_________________________________________________________________
</pre>

<p>For later convenience, let us encapsulate all of this logic in a function we can use to get model constructors for fixed hyperparameters. The returned constructor by convention takes only the <code class="language-plaintext highlighter-rouge">GraphTensorSpec</code> of the input graphs for the model, and for good measure our constructor will also add some \(L_2\) regularization through an <code class="language-plaintext highlighter-rouge">l2_coefficient</code> hyperparameter:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_model_creation_fn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hops</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">l2_coefficient</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Return a model constructor for a given set of hyperparameters.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">model_creation_fn</span><span class="p">(</span><span class="n">graph_tensor_spec</span><span class="p">):</span>
        <span class="n">initial_map_features</span> <span class="o">=</span> <span class="nf">get_initial_map_features</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
        <span class="n">mpnn</span> <span class="o">=</span> <span class="nc">MPNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hops</span><span class="o">=</span><span class="n">hops</span><span class="p">)</span>
        
        <span class="n">model</span> <span class="o">=</span> <span class="nf">vanilla_mpnn_model</span><span class="p">(</span><span class="n">graph_tensor_spec</span><span class="o">=</span><span class="n">graph_tensor_spec</span><span class="p">,</span>
                                   <span class="n">init_states_fn</span><span class="o">=</span><span class="n">initial_map_features</span><span class="p">,</span>
                                   <span class="n">pass_messages_fn</span><span class="o">=</span><span class="n">mpnn</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">add_loss</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">([</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="nf">l2</span><span class="p">(</span><span class="n">l2</span><span class="o">=</span><span class="n">l2_coefficient</span><span class="p">)(</span><span class="n">weight</span><span class="p">)</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_weights</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">model</span>
    <span class="k">return</span> <span class="n">model_creation_fn</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">mpnn_creation_fn</span> <span class="o">=</span> <span class="nf">get_model_creation_fn</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">hops</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="nf">mpnn_creation_fn</span><span class="p">(</span><span class="n">graph_spec</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span></code></pre></figure>

<pre class="output">
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [()]                      0         
                                                                 
 graph_embedding (MapFeature  ()                       5248      
 s)                                                              
                                                                 
 gat_mpnn (MPNN)             ()                        396288    
                                                                 
=================================================================
Total params: 401,536
Trainable params: 401,536
Non-trainable params: 0
_________________________________________________________________
</pre>

<div class="section separator"></div>

<h2 id="4-graph-binary-classification"><a id="classification">4. Graph binary classification</a></h2>

<div class="subsection separator"></div>

<h3 id="41-task-specification"><a id="classification_task">4.1 Task specification</a></h3>

<p>Having a GNN model at our disposal, we are now ready to apply it to the task at hand, namely the binary classification of molecules predicting their toxicity. This involves:</p>

<ol>
  <li>Adding a readout and prediction head, which computes logits for each class from the features computed by the GNN.</li>
  <li>Defining the loss function to be minimized, which in this case should be a categorical crossentropy loss.</li>
  <li>Defining the metrics we are interested in measuring during training and validation.</li>
</ol>

<p>The <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md">orchestrator</a> defines the <code class="language-plaintext highlighter-rouge">Task</code> protocol to achieve these goals, and conveniently provides a pre-implemented <code class="language-plaintext highlighter-rouge">GraphBinaryClassification</code> class which complies with this protocol. While we could use it as is, for illustration purposes here we will extend its basic implementation in two ways:</p>
<ul>
  <li>We will include the AUROC metric, which is reported by the authors of <a href="#cardiotox">[1]</a> (other metrics reported there can be added in a similar fashion).</li>
  <li>We will generalize the readout and prediction head to include a hidden layer.</li>
</ul>

<p>First, we define a simple wrapper around the <code class="language-plaintext highlighter-rouge">tf.keras.metrics.AUC</code> class to adapt it to our conventions:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">AUROC</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="n">AUC</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    AUROC metric computation for binary classification from logits.
    
    y_true: true labels, with shape (batch_size,)
    y_pred: predicted logits, with shape (batch_size, 2)
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">update_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">update_state</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">])</span></code></pre></figure>

<p>Next, we subclass the <code class="language-plaintext highlighter-rouge">GraphBinaryClassification</code> task and override its <code class="language-plaintext highlighter-rouge">adapt</code> and <code class="language-plaintext highlighter-rouge">metrics</code> methods:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">GraphBinaryClassification</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">GraphBinaryClassification</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    A GraphBinaryClassification task with a hidden layer in the prediction head, and additional metrics.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">adapt</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">tfgnn</span><span class="p">.</span><span class="nf">pool_nodes_to_context</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">output</span><span class="p">,</span>
                                                   <span class="n">node_set_name</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">_node_set_name</span><span class="p">,</span>
                                                   <span class="n">reduce_type</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">_reduce_type</span><span class="p">,</span>
                                                   <span class="n">feature_name</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">_state_name</span><span class="p">)</span>
        
        <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">_hidden_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">hidden_layer</span><span class="sh">'</span><span class="p">)(</span><span class="n">hidden_state</span><span class="p">)</span>
        
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">_units</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">logits</span><span class="sh">'</span><span class="p">)(</span><span class="n">hidden_state</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">metrics</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">return </span><span class="p">(</span><span class="o">*</span><span class="nf">super</span><span class="p">().</span><span class="nf">metrics</span><span class="p">(),</span> <span class="nc">AUROC</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">AUROC</span><span class="sh">'</span><span class="p">))</span></code></pre></figure>

<p>To create an instance of this class we need to specify the node set which will be used to aggregate hidden states for prediction (remember in our case there is only one, <code class="language-plaintext highlighter-rouge">'atom'</code>) and the number of classes (two, for toxic and non-toxic), as well as the new hyperparameter <code class="language-plaintext highlighter-rouge">hidden_dim</code>:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">task</span> <span class="o">=</span> <span class="nc">GraphBinaryClassification</span><span class="p">(</span><span class="n">hidden_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">node_set_name</span><span class="o">=</span><span class="sh">'</span><span class="s">atom</span><span class="sh">'</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span></code></pre></figure>

<p>This instance then provides everything we will need for training, namely:</p>
<ul>
  <li>The loss function</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">task</span><span class="p">.</span><span class="nf">losses</span><span class="p">()</span></code></pre></figure>

<pre class="output">
(&lt;keras.losses.SparseCategoricalCrossentropy at 0x7fe2bc861650&gt;,)
</pre>

<ul>
  <li>The metrics</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">task</span><span class="p">.</span><span class="nf">metrics</span><span class="p">()</span></code></pre></figure>

<pre class="output">
(&lt;keras.metrics.SparseCategoricalAccuracy at 0x7fe2bc795550&gt;,
 &lt;keras.metrics.SparseCategoricalCrossentropy at 0x7fe2bc861850&gt;,
 &lt;__main__.AUROC at 0x7fe2bc90a590&gt;)
</pre>

<ul>
  <li>An <code class="language-plaintext highlighter-rouge">adapt</code> method to place the readout and prediction head on top of the GNN</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">classification_model</span> <span class="o">=</span> <span class="n">task</span><span class="p">.</span><span class="nf">adapt</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">classification_model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span></code></pre></figure>

<pre class="output">
Model: "model_4"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_4 (InputLayer)           [()]                 0           []                               
                                                                                                  
 graph_embedding (MapFeatures)  ()                   5248        ['input_4[0][0]']                
                                                                                                  
 gat_mpnn (MPNN)                ()                   396288      ['graph_embedding[0][0]']        
                                                                                                  
 input.node_sets (InstancePrope  {'atom': ()}        0           ['gat_mpnn[0][0]']               
 rty)                                                                                             
                                                                                                  
 input.sizes (InstanceProperty)  (1,)                0           ['input.node_sets[0][0]']        
                                                                                                  
 tf.math.cumsum (TFOpLambda)    (1,)                 0           ['input.sizes[0][0]']            
                                                                                                  
 tf.math.reduce_sum (TFOpLambda  ()                  0           ['input.sizes[0][0]']            
 )                                                                                                
                                                                                                  
 tf.ones_like (TFOpLambda)      (1,)                 0           ['tf.math.cumsum[0][0]']         
                                                                                                  
 tf.__operators__.add (TFOpLamb  ()                  0           ['tf.math.reduce_sum[0][0]']     
 da)                                                                                              
                                                                                                  
 tf.math.unsorted_segment_sum (  (None,)             0           ['tf.ones_like[0][0]',           
 TFOpLambda)                                                      'tf.__operators__.add[0][0]',   
                                                                  'tf.math.cumsum[0][0]']         
                                                                                                  
 tf.math.cumsum_1 (TFOpLambda)  (None,)              0           ['tf.math.unsorted_segment_sum[0]
                                                                 [0]']                            
                                                                                                  
 input._get_features_ref_4 (Ins  {'hidden_state': (N  0          ['input.node_sets[0][0]']        
 tanceProperty)                 one, 128)}                                                        
                                                                                                  
 tf.__operators__.getitem (Slic  (None,)             0           ['tf.math.cumsum_1[0][0]',       
 ingOpLambda)                                                     'tf.math.reduce_sum[0][0]']     
                                                                                                  
 tf.math.unsorted_segment_mean   (1, 128)            0           ['input._get_features_ref_4[0][0]
 (TFOpLambda)                                                    ',                               
                                                                  'tf.__operators__.getitem[0][0]'
                                                                 ]                                
                                                                                                  
 hidden_layer (Dense)           (1, 256)             33024       ['tf.math.unsorted_segment_mean[0
                                                                 ][0]']                           
                                                                                                  
 logits (Dense)                 (1, 2)               514         ['hidden_layer[0][0]']           
                                                                                                  
==================================================================================================
Total params: 435,074
Trainable params: 435,074
Non-trainable params: 0
________________________________________________________________________________________________
</pre>

<p>The resulting model then produces logits for each class, given a <code class="language-plaintext highlighter-rouge">GraphTensor</code> as its input:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nf">classification_model</span><span class="p">(</span><span class="n">graph_tensor</span><span class="p">)</span></code></pre></figure>

<pre class="output">
&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-0.05114917, -0.23557672]], dtype=float32)&gt;
</pre>

<ul>
  <li>A <code class="language-plaintext highlighter-rouge">preprocessors</code> method, which could be used to pre-process the graphs before reaching the GNN but will remain unused here</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">task</span><span class="p">.</span><span class="nf">preprocessors</span><span class="p">()</span></code></pre></figure>

<pre class="output">
()
</pre>

<h3 id="42-training"><a id="classification_training">4.2 Training</a></h3>

<p>We are now ready to train the model. First, we create a <code class="language-plaintext highlighter-rouge">KerasTrainer</code> instance which implements the <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md#training">orchestrator’s <code class="language-plaintext highlighter-rouge">Trainer</code> protocol</a> making use of Keras’ <code class="language-plaintext highlighter-rouge">fit</code> method:<a id="strategy"></a></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">trainer</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="nc">KerasTrainer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">distribute</span><span class="p">.</span><span class="nf">get_strategy</span><span class="p">(),</span> <span class="n">model_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<p>Next, we define a simple function conforming to <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md#graphtensor-processing">the <code class="language-plaintext highlighter-rouge">GraphTensorProcessorFn</code> protocol</a>, which extracts the labels from the <code class="language-plaintext highlighter-rouge">GraphTensor</code> objects, for use during supervised training (this function will be mapped over the datasets that are then passed on to the <code class="language-plaintext highlighter-rouge">tf.keras.Model.fit</code> method):</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">extract_labels</span><span class="p">(</span><span class="n">graph_tensor</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Extract the toxicity class label from the `GraphTensor` representation of a molecule.
    Return a pair compatible with the `tf.keras.Model.fit` method.
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">graph_tensor</span><span class="p">,</span> <span class="n">graph_tensor</span><span class="p">.</span><span class="n">context</span><span class="p">[</span><span class="sh">'</span><span class="s">toxicity</span><span class="sh">'</span><span class="p">]</span></code></pre></figure>

<p>Lastly, we can put everything together and get a coffee while watching some progress bars move :-)<a id="orchestrator"></a></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">runner</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span>
    <span class="n">train_ds_provider</span><span class="o">=</span><span class="n">train_dataset_provider</span><span class="p">,</span>
    <span class="n">valid_ds_provider</span><span class="o">=</span><span class="n">valid_dataset_provider</span><span class="p">,</span>
    <span class="n">feature_processors</span><span class="o">=</span><span class="p">[</span><span class="n">extract_labels</span><span class="p">],</span>
    <span class="n">model_fn</span><span class="o">=</span><span class="nf">get_model_creation_fn</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">hops</span><span class="o">=</span><span class="mi">8</span><span class="p">),</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">optimizer_fn</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">,</span>
    <span class="n">gtspec</span><span class="o">=</span><span class="n">graph_spec</span><span class="p">,</span>
    <span class="n">global_batch_size</span><span class="o">=</span><span class="mi">128</span>
<span class="p">)</span></code></pre></figure>

<pre class="output">
Epoch 1/50
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_7/node_set_update_7/gat_v2_conv/Reshape_3:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_7/node_set_update_7/gat_v2_conv/Reshape_2:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_7/node_set_update_7/gat_v2_conv/Cast:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_7/node_set_update_7/gat_v2_conv/Reshape_6:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_7/node_set_update_7/gat_v2_conv/Reshape_5:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_7/node_set_update_7/gat_v2_conv/Cast_1:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_6/node_set_update_6/gat_v2_conv/Reshape_3:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_6/node_set_update_6/gat_v2_conv/Reshape_2:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_6/node_set_update_6/gat_v2_conv/Cast:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_6/node_set_update_6/gat_v2_conv/Reshape_6:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_6/node_set_update_6/gat_v2_conv/Reshape_5:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_6/node_set_update_6/gat_v2_conv/Cast_1:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_5/node_set_update_5/gat_v2_conv/Reshape_3:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_5/node_set_update_5/gat_v2_conv/Reshape_2:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_5/node_set_update_5/gat_v2_conv/Cast:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_5/node_set_update_5/gat_v2_conv/Reshape_6:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_5/node_set_update_5/gat_v2_conv/Reshape_5:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_5/node_set_update_5/gat_v2_conv/Cast_1:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_4/node_set_update_4/gat_v2_conv/Reshape_3:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_4/node_set_update_4/gat_v2_conv/Reshape_2:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_4/node_set_update_4/gat_v2_conv/Cast:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_4/node_set_update_4/gat_v2_conv/Reshape_6:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_4/node_set_update_4/gat_v2_conv/Reshape_5:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_4/node_set_update_4/gat_v2_conv/Cast_1:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_3/node_set_update_3/gat_v2_conv/Reshape_3:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_3/node_set_update_3/gat_v2_conv/Reshape_2:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_3/node_set_update_3/gat_v2_conv/Cast:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_3/node_set_update_3/gat_v2_conv/Reshape_6:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_3/node_set_update_3/gat_v2_conv/Reshape_5:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_3/node_set_update_3/gat_v2_conv/Cast_1:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_2/node_set_update_2/gat_v2_conv/Reshape_3:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_2/node_set_update_2/gat_v2_conv/Reshape_2:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_2/node_set_update_2/gat_v2_conv/Cast:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_2/node_set_update_2/gat_v2_conv/Reshape_6:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_2/node_set_update_2/gat_v2_conv/Reshape_5:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_2/node_set_update_2/gat_v2_conv/Cast_1:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_1/node_set_update_1/gat_v2_conv/Reshape_3:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_1/node_set_update_1/gat_v2_conv/Reshape_2:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_1/node_set_update_1/gat_v2_conv/Cast:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_1/node_set_update_1/gat_v2_conv/Reshape_6:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_1/node_set_update_1/gat_v2_conv/Reshape_5:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_1/node_set_update_1/gat_v2_conv/Cast_1:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_0/node_set_update/gat_v2_conv/Reshape_3:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_0/node_set_update/gat_v2_conv/Reshape_2:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_0/node_set_update/gat_v2_conv/Cast:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_0/node_set_update/gat_v2_conv/Reshape_6:0", shape=(None,), dtype=int32), values=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_0/node_set_update/gat_v2_conv/Reshape_5:0", shape=(None, 1, 1), dtype=float32), dense_shape=Tensor("gradient_tape/model_7/gat_mpnn/message_passing_0/node_set_update/gat_v2_conv/Cast_1:0", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "shape. This may consume a large amount of memory." % value)
50/50 [==============================] - 10s 89ms/step - loss: 0.5478 - sparse_categorical_accuracy: 0.7352 - sparse_categorical_crossentropy: 0.5478 - AUROC: 0.6414 - val_loss: 0.5037 - val_sparse_categorical_accuracy: 0.7311 - val_sparse_categorical_crossentropy: 0.5037 - val_AUROC: 0.7434
Epoch 2/50
50/50 [==============================] - 3s 50ms/step - loss: 0.4676 - sparse_categorical_accuracy: 0.7645 - sparse_categorical_crossentropy: 0.4676 - AUROC: 0.7896 - val_loss: 0.4758 - val_sparse_categorical_accuracy: 0.7520 - val_sparse_categorical_crossentropy: 0.4758 - val_AUROC: 0.7870
Epoch 3/50
50/50 [==============================] - 2s 50ms/step - loss: 0.4404 - sparse_categorical_accuracy: 0.7889 - sparse_categorical_crossentropy: 0.4404 - AUROC: 0.8219 - val_loss: 0.4469 - val_sparse_categorical_accuracy: 0.7884 - val_sparse_categorical_crossentropy: 0.4469 - val_AUROC: 0.8204
Epoch 4/50
50/50 [==============================] - 2s 49ms/step - loss: 0.4192 - sparse_categorical_accuracy: 0.8055 - sparse_categorical_crossentropy: 0.4192 - AUROC: 0.8423 - val_loss: 0.4276 - val_sparse_categorical_accuracy: 0.8027 - val_sparse_categorical_crossentropy: 0.4276 - val_AUROC: 0.8406
Epoch 5/50
50/50 [==============================] - 3s 58ms/step - loss: 0.4015 - sparse_categorical_accuracy: 0.8191 - sparse_categorical_crossentropy: 0.4015 - AUROC: 0.8577 - val_loss: 0.4257 - val_sparse_categorical_accuracy: 0.8079 - val_sparse_categorical_crossentropy: 0.4257 - val_AUROC: 0.8441
Epoch 6/50
50/50 [==============================] - 3s 50ms/step - loss: 0.3943 - sparse_categorical_accuracy: 0.8202 - sparse_categorical_crossentropy: 0.3943 - AUROC: 0.8631 - val_loss: 0.4186 - val_sparse_categorical_accuracy: 0.8099 - val_sparse_categorical_crossentropy: 0.4186 - val_AUROC: 0.8497
Epoch 7/50
50/50 [==============================] - 2s 50ms/step - loss: 0.3878 - sparse_categorical_accuracy: 0.8286 - sparse_categorical_crossentropy: 0.3878 - AUROC: 0.8675 - val_loss: 0.4138 - val_sparse_categorical_accuracy: 0.8132 - val_sparse_categorical_crossentropy: 0.4138 - val_AUROC: 0.8529
Epoch 8/50
50/50 [==============================] - 3s 54ms/step - loss: 0.3860 - sparse_categorical_accuracy: 0.8322 - sparse_categorical_crossentropy: 0.3860 - AUROC: 0.8687 - val_loss: 0.4108 - val_sparse_categorical_accuracy: 0.8145 - val_sparse_categorical_crossentropy: 0.4108 - val_AUROC: 0.8522
Epoch 9/50
50/50 [==============================] - 2s 50ms/step - loss: 0.3823 - sparse_categorical_accuracy: 0.8327 - sparse_categorical_crossentropy: 0.3823 - AUROC: 0.8717 - val_loss: 0.4095 - val_sparse_categorical_accuracy: 0.8125 - val_sparse_categorical_crossentropy: 0.4095 - val_AUROC: 0.8551
Epoch 10/50
50/50 [==============================] - 3s 52ms/step - loss: 0.3759 - sparse_categorical_accuracy: 0.8350 - sparse_categorical_crossentropy: 0.3759 - AUROC: 0.8762 - val_loss: 0.4053 - val_sparse_categorical_accuracy: 0.8171 - val_sparse_categorical_crossentropy: 0.4053 - val_AUROC: 0.8586
Epoch 11/50
50/50 [==============================] - 2s 47ms/step - loss: 0.3726 - sparse_categorical_accuracy: 0.8363 - sparse_categorical_crossentropy: 0.3726 - AUROC: 0.8780 - val_loss: 0.4177 - val_sparse_categorical_accuracy: 0.8145 - val_sparse_categorical_crossentropy: 0.4177 - val_AUROC: 0.8496
Epoch 12/50
50/50 [==============================] - 2s 49ms/step - loss: 0.3692 - sparse_categorical_accuracy: 0.8389 - sparse_categorical_crossentropy: 0.3692 - AUROC: 0.8800 - val_loss: 0.3957 - val_sparse_categorical_accuracy: 0.8184 - val_sparse_categorical_crossentropy: 0.3957 - val_AUROC: 0.8635
Epoch 13/50
50/50 [==============================] - 2s 48ms/step - loss: 0.3590 - sparse_categorical_accuracy: 0.8458 - sparse_categorical_crossentropy: 0.3590 - AUROC: 0.8871 - val_loss: 0.3963 - val_sparse_categorical_accuracy: 0.8216 - val_sparse_categorical_crossentropy: 0.3963 - val_AUROC: 0.8652
Epoch 14/50
50/50 [==============================] - 3s 54ms/step - loss: 0.3579 - sparse_categorical_accuracy: 0.8466 - sparse_categorical_crossentropy: 0.3579 - AUROC: 0.8877 - val_loss: 0.3913 - val_sparse_categorical_accuracy: 0.8294 - val_sparse_categorical_crossentropy: 0.3913 - val_AUROC: 0.8689
Epoch 15/50
50/50 [==============================] - 3s 54ms/step - loss: 0.3519 - sparse_categorical_accuracy: 0.8519 - sparse_categorical_crossentropy: 0.3519 - AUROC: 0.8916 - val_loss: 0.3798 - val_sparse_categorical_accuracy: 0.8359 - val_sparse_categorical_crossentropy: 0.3798 - val_AUROC: 0.8759
Epoch 16/50
50/50 [==============================] - 2s 48ms/step - loss: 0.3451 - sparse_categorical_accuracy: 0.8536 - sparse_categorical_crossentropy: 0.3451 - AUROC: 0.8961 - val_loss: 0.3811 - val_sparse_categorical_accuracy: 0.8372 - val_sparse_categorical_crossentropy: 0.3811 - val_AUROC: 0.8737
Epoch 17/50
50/50 [==============================] - 2s 49ms/step - loss: 0.3378 - sparse_categorical_accuracy: 0.8580 - sparse_categorical_crossentropy: 0.3378 - AUROC: 0.9010 - val_loss: 0.3823 - val_sparse_categorical_accuracy: 0.8294 - val_sparse_categorical_crossentropy: 0.3823 - val_AUROC: 0.8750
Epoch 18/50
50/50 [==============================] - 2s 48ms/step - loss: 0.3436 - sparse_categorical_accuracy: 0.8573 - sparse_categorical_crossentropy: 0.3436 - AUROC: 0.8964 - val_loss: 0.3918 - val_sparse_categorical_accuracy: 0.8294 - val_sparse_categorical_crossentropy: 0.3918 - val_AUROC: 0.8753
Epoch 19/50
50/50 [==============================] - 2s 48ms/step - loss: 0.3439 - sparse_categorical_accuracy: 0.8562 - sparse_categorical_crossentropy: 0.3439 - AUROC: 0.8969 - val_loss: 0.3861 - val_sparse_categorical_accuracy: 0.8294 - val_sparse_categorical_crossentropy: 0.3861 - val_AUROC: 0.8746
Epoch 20/50
50/50 [==============================] - 2s 47ms/step - loss: 0.3337 - sparse_categorical_accuracy: 0.8612 - sparse_categorical_crossentropy: 0.3337 - AUROC: 0.9032 - val_loss: 0.3891 - val_sparse_categorical_accuracy: 0.8210 - val_sparse_categorical_crossentropy: 0.3891 - val_AUROC: 0.8750
Epoch 21/50
50/50 [==============================] - 2s 49ms/step - loss: 0.3286 - sparse_categorical_accuracy: 0.8661 - sparse_categorical_crossentropy: 0.3286 - AUROC: 0.9056 - val_loss: 0.3782 - val_sparse_categorical_accuracy: 0.8464 - val_sparse_categorical_crossentropy: 0.3782 - val_AUROC: 0.8756
Epoch 22/50
50/50 [==============================] - 3s 51ms/step - loss: 0.3286 - sparse_categorical_accuracy: 0.8658 - sparse_categorical_crossentropy: 0.3286 - AUROC: 0.9066 - val_loss: 0.3853 - val_sparse_categorical_accuracy: 0.8294 - val_sparse_categorical_crossentropy: 0.3853 - val_AUROC: 0.8775
Epoch 23/50
50/50 [==============================] - 2s 47ms/step - loss: 0.3229 - sparse_categorical_accuracy: 0.8692 - sparse_categorical_crossentropy: 0.3229 - AUROC: 0.9090 - val_loss: 0.3849 - val_sparse_categorical_accuracy: 0.8346 - val_sparse_categorical_crossentropy: 0.3849 - val_AUROC: 0.8746
Epoch 24/50
50/50 [==============================] - 2s 48ms/step - loss: 0.3130 - sparse_categorical_accuracy: 0.8747 - sparse_categorical_crossentropy: 0.3130 - AUROC: 0.9151 - val_loss: 0.3758 - val_sparse_categorical_accuracy: 0.8398 - val_sparse_categorical_crossentropy: 0.3758 - val_AUROC: 0.8801
Epoch 25/50
50/50 [==============================] - 2s 50ms/step - loss: 0.3132 - sparse_categorical_accuracy: 0.8756 - sparse_categorical_crossentropy: 0.3132 - AUROC: 0.9146 - val_loss: 0.3712 - val_sparse_categorical_accuracy: 0.8431 - val_sparse_categorical_crossentropy: 0.3712 - val_AUROC: 0.8833
Epoch 26/50
50/50 [==============================] - 3s 66ms/step - loss: 0.3085 - sparse_categorical_accuracy: 0.8766 - sparse_categorical_crossentropy: 0.3085 - AUROC: 0.9181 - val_loss: 0.3681 - val_sparse_categorical_accuracy: 0.8470 - val_sparse_categorical_crossentropy: 0.3681 - val_AUROC: 0.8848
Epoch 27/50
50/50 [==============================] - 3s 51ms/step - loss: 0.2998 - sparse_categorical_accuracy: 0.8797 - sparse_categorical_crossentropy: 0.2998 - AUROC: 0.9225 - val_loss: 0.3674 - val_sparse_categorical_accuracy: 0.8496 - val_sparse_categorical_crossentropy: 0.3674 - val_AUROC: 0.8859
Epoch 28/50
50/50 [==============================] - 2s 49ms/step - loss: 0.3059 - sparse_categorical_accuracy: 0.8789 - sparse_categorical_crossentropy: 0.3059 - AUROC: 0.9190 - val_loss: 0.3640 - val_sparse_categorical_accuracy: 0.8548 - val_sparse_categorical_crossentropy: 0.3640 - val_AUROC: 0.8892
Epoch 29/50
50/50 [==============================] - 3s 52ms/step - loss: 0.2951 - sparse_categorical_accuracy: 0.8828 - sparse_categorical_crossentropy: 0.2951 - AUROC: 0.9240 - val_loss: 0.3736 - val_sparse_categorical_accuracy: 0.8483 - val_sparse_categorical_crossentropy: 0.3736 - val_AUROC: 0.8828
Epoch 30/50
50/50 [==============================] - 2s 49ms/step - loss: 0.2938 - sparse_categorical_accuracy: 0.8825 - sparse_categorical_crossentropy: 0.2938 - AUROC: 0.9254 - val_loss: 0.3759 - val_sparse_categorical_accuracy: 0.8470 - val_sparse_categorical_crossentropy: 0.3759 - val_AUROC: 0.8827
Epoch 31/50
50/50 [==============================] - 2s 49ms/step - loss: 0.2848 - sparse_categorical_accuracy: 0.8856 - sparse_categorical_crossentropy: 0.2848 - AUROC: 0.9301 - val_loss: 0.3668 - val_sparse_categorical_accuracy: 0.8451 - val_sparse_categorical_crossentropy: 0.3668 - val_AUROC: 0.8877
Epoch 32/50
50/50 [==============================] - 3s 58ms/step - loss: 0.2885 - sparse_categorical_accuracy: 0.8861 - sparse_categorical_crossentropy: 0.2885 - AUROC: 0.9275 - val_loss: 0.3867 - val_sparse_categorical_accuracy: 0.8294 - val_sparse_categorical_crossentropy: 0.3867 - val_AUROC: 0.8832
Epoch 33/50
50/50 [==============================] - 2s 48ms/step - loss: 0.2904 - sparse_categorical_accuracy: 0.8831 - sparse_categorical_crossentropy: 0.2904 - AUROC: 0.9269 - val_loss: 0.3913 - val_sparse_categorical_accuracy: 0.8372 - val_sparse_categorical_crossentropy: 0.3913 - val_AUROC: 0.8765
Epoch 34/50
50/50 [==============================] - 2s 47ms/step - loss: 0.2891 - sparse_categorical_accuracy: 0.8834 - sparse_categorical_crossentropy: 0.2891 - AUROC: 0.9276 - val_loss: 0.3857 - val_sparse_categorical_accuracy: 0.8444 - val_sparse_categorical_crossentropy: 0.3857 - val_AUROC: 0.8809
Epoch 35/50
50/50 [==============================] - 2s 50ms/step - loss: 0.2780 - sparse_categorical_accuracy: 0.8894 - sparse_categorical_crossentropy: 0.2780 - AUROC: 0.9333 - val_loss: 0.3794 - val_sparse_categorical_accuracy: 0.8470 - val_sparse_categorical_crossentropy: 0.3794 - val_AUROC: 0.8894
Epoch 36/50
50/50 [==============================] - 3s 57ms/step - loss: 0.2670 - sparse_categorical_accuracy: 0.8945 - sparse_categorical_crossentropy: 0.2670 - AUROC: 0.9382 - val_loss: 0.4007 - val_sparse_categorical_accuracy: 0.8197 - val_sparse_categorical_crossentropy: 0.4007 - val_AUROC: 0.8775
Epoch 37/50
50/50 [==============================] - 3s 52ms/step - loss: 0.2598 - sparse_categorical_accuracy: 0.8964 - sparse_categorical_crossentropy: 0.2598 - AUROC: 0.9422 - val_loss: 0.3599 - val_sparse_categorical_accuracy: 0.8542 - val_sparse_categorical_crossentropy: 0.3599 - val_AUROC: 0.8921
Epoch 38/50
50/50 [==============================] - 3s 55ms/step - loss: 0.2585 - sparse_categorical_accuracy: 0.8959 - sparse_categorical_crossentropy: 0.2585 - AUROC: 0.9428 - val_loss: 0.3738 - val_sparse_categorical_accuracy: 0.8529 - val_sparse_categorical_crossentropy: 0.3738 - val_AUROC: 0.8895
Epoch 39/50
50/50 [==============================] - 2s 47ms/step - loss: 0.2507 - sparse_categorical_accuracy: 0.9002 - sparse_categorical_crossentropy: 0.2507 - AUROC: 0.9458 - val_loss: 0.3730 - val_sparse_categorical_accuracy: 0.8561 - val_sparse_categorical_crossentropy: 0.3730 - val_AUROC: 0.8937
Epoch 40/50
50/50 [==============================] - 2s 49ms/step - loss: 0.2546 - sparse_categorical_accuracy: 0.8975 - sparse_categorical_crossentropy: 0.2546 - AUROC: 0.9446 - val_loss: 0.3717 - val_sparse_categorical_accuracy: 0.8535 - val_sparse_categorical_crossentropy: 0.3717 - val_AUROC: 0.8954
Epoch 41/50
50/50 [==============================] - 2s 50ms/step - loss: 0.2463 - sparse_categorical_accuracy: 0.9020 - sparse_categorical_crossentropy: 0.2463 - AUROC: 0.9484 - val_loss: 0.3772 - val_sparse_categorical_accuracy: 0.8470 - val_sparse_categorical_crossentropy: 0.3772 - val_AUROC: 0.8899
Epoch 42/50
50/50 [==============================] - 2s 47ms/step - loss: 0.2490 - sparse_categorical_accuracy: 0.9028 - sparse_categorical_crossentropy: 0.2490 - AUROC: 0.9459 - val_loss: 0.3963 - val_sparse_categorical_accuracy: 0.8477 - val_sparse_categorical_crossentropy: 0.3963 - val_AUROC: 0.8955
Epoch 43/50
50/50 [==============================] - 2s 48ms/step - loss: 0.2403 - sparse_categorical_accuracy: 0.9038 - sparse_categorical_crossentropy: 0.2403 - AUROC: 0.9498 - val_loss: 0.3779 - val_sparse_categorical_accuracy: 0.8522 - val_sparse_categorical_crossentropy: 0.3779 - val_AUROC: 0.9005
Epoch 44/50
50/50 [==============================] - 2s 48ms/step - loss: 0.2527 - sparse_categorical_accuracy: 0.8994 - sparse_categorical_crossentropy: 0.2527 - AUROC: 0.9447 - val_loss: 0.3837 - val_sparse_categorical_accuracy: 0.8405 - val_sparse_categorical_crossentropy: 0.3837 - val_AUROC: 0.8924
Epoch 45/50
50/50 [==============================] - 3s 53ms/step - loss: 0.2330 - sparse_categorical_accuracy: 0.9084 - sparse_categorical_crossentropy: 0.2330 - AUROC: 0.9539 - val_loss: 0.3765 - val_sparse_categorical_accuracy: 0.8652 - val_sparse_categorical_crossentropy: 0.3765 - val_AUROC: 0.8977
Epoch 46/50
50/50 [==============================] - 3s 52ms/step - loss: 0.2286 - sparse_categorical_accuracy: 0.9127 - sparse_categorical_crossentropy: 0.2286 - AUROC: 0.9541 - val_loss: 0.3828 - val_sparse_categorical_accuracy: 0.8535 - val_sparse_categorical_crossentropy: 0.3828 - val_AUROC: 0.8976
Epoch 47/50
50/50 [==============================] - 2s 48ms/step - loss: 0.2172 - sparse_categorical_accuracy: 0.9156 - sparse_categorical_crossentropy: 0.2172 - AUROC: 0.9594 - val_loss: 0.4157 - val_sparse_categorical_accuracy: 0.8457 - val_sparse_categorical_crossentropy: 0.4157 - val_AUROC: 0.8968
Epoch 48/50
50/50 [==============================] - 2s 48ms/step - loss: 0.2285 - sparse_categorical_accuracy: 0.9142 - sparse_categorical_crossentropy: 0.2285 - AUROC: 0.9549 - val_loss: 0.4125 - val_sparse_categorical_accuracy: 0.8509 - val_sparse_categorical_crossentropy: 0.4125 - val_AUROC: 0.8921
Epoch 49/50
50/50 [==============================] - 3s 52ms/step - loss: 0.2226 - sparse_categorical_accuracy: 0.9144 - sparse_categorical_crossentropy: 0.2226 - AUROC: 0.9574 - val_loss: 0.3726 - val_sparse_categorical_accuracy: 0.8620 - val_sparse_categorical_crossentropy: 0.3726 - val_AUROC: 0.9007
Epoch 50/50
50/50 [==============================] - 2s 48ms/step - loss: 0.2209 - sparse_categorical_accuracy: 0.9155 - sparse_categorical_crossentropy: 0.2209 - AUROC: 0.9583 - val_loss: 0.3729 - val_sparse_categorical_accuracy: 0.8600 - val_sparse_categorical_crossentropy: 0.3729 - val_AUROC: 0.9016
2022-06-23 19:33:36.568640: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:524: UserWarning: Encoding a StructuredValue with type tensorflow_gnn.GraphTensorSpec; loading this StructuredValue will require that this type be imported and registered.
  "imported and registered." % type_spec_class_name)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:524: UserWarning: Encoding a StructuredValue with type tensorflow_gnn.ContextSpec.v2; loading this StructuredValue will require that this type be imported and registered.
  "imported and registered." % type_spec_class_name)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:524: UserWarning: Encoding a StructuredValue with type tensorflow_gnn.NodeSetSpec; loading this StructuredValue will require that this type be imported and registered.
  "imported and registered." % type_spec_class_name)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:524: UserWarning: Encoding a StructuredValue with type tensorflow_gnn.EdgeSetSpec; loading this StructuredValue will require that this type be imported and registered.
  "imported and registered." % type_spec_class_name)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:524: UserWarning: Encoding a StructuredValue with type tensorflow_gnn.AdjacencySpec; loading this StructuredValue will require that this type be imported and registered.
  "imported and registered." % type_spec_class_name)
</pre>

<h3 id="43-metric-visualization"><a id="classification_metrics">4.3 Metric visualization</a></h3>

<p>A straightforward way to visualize the various metrics collected during training and validation is to use TensorBoard. Ideally, the following magic should work in your system:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">%</span><span class="n">load_ext</span> <span class="n">tensorboard</span>
<span class="o">%</span><span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span> <span class="n">model</span> <span class="o">--</span><span class="n">bind_all</span></code></pre></figure>

<p>For reference, I have uploaded the logs of a local run to <a href="https://tensorboard.dev/experiment/Y85x93T8Rl6p5QqyajLFeQ/#scalars">TensorBoard.dev</a>. There you should see that the loss decreases until we start to overfit (orange line is training, blue line is validation):</p>

<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog/tfgnn-intro/epoch_loss-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog/tfgnn-intro/epoch_loss-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog/tfgnn-intro/epoch_loss-1400.webp" />
    <!-- Fallback to the original file -->
    <!-- style="max-width:100%;max-height:100%" -->
    <img src="/assets/img/blog/tfgnn-intro/epoch_loss.png" />

  </picture>

</figure>

<p>The accuracy reaches approximately 87%</p>

<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog/tfgnn-intro/accuracy-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog/tfgnn-intro/accuracy-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog/tfgnn-intro/accuracy-1400.webp" />
    <!-- Fallback to the original file -->
    <!-- style="max-width:100%;max-height:100%" -->
    <img src="/assets/img/blog/tfgnn-intro/accuracy.png" />

  </picture>

</figure>

<p>And the AUROC increases up to approximately 0.9</p>

<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog/tfgnn-intro/auroc-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog/tfgnn-intro/auroc-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog/tfgnn-intro/auroc-1400.webp" />
    <!-- Fallback to the original file -->
    <!-- style="max-width:100%;max-height:100%" -->
    <img src="/assets/img/blog/tfgnn-intro/auroc.png" />

  </picture>

</figure>

<div class="section separator"></div>

<h2 id="conclusions"><a id="conclusions">Conclusions</a></h2>

<p>In this notebook we have seen how to train a GNN model for graph binary classification using TF-GNN in an end-to-end fashion. The final <a href="#orchestrator">cell running the orchestrator</a> brings together all the elements we introduced along the way, namely:</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">DatasetProvider</code>-compliant objects <code class="language-plaintext highlighter-rouge">train_dataset_provider</code> and <code class="language-plaintext highlighter-rouge">valid_dataset_provider</code> constructed in <a href="#dataprep">§2</a> to provide the data</li>
  <li>The model constructor function <code class="language-plaintext highlighter-rouge">get_model_creation_fn</code> built in <a href="#mpnn_construction">§3.3</a> with the components of <a href="#mpnn_embedding">§3.1</a> and <a href="#mpnn_stack">§3.2</a> to assemble the GNN</li>
  <li>The <code class="language-plaintext highlighter-rouge">GraphBinaryClassification</code> task defined in <a href="#classification_task">§4.1</a> to specify the readout and prediction head, as well as the loss and metrics.</li>
  <li>The <code class="language-plaintext highlighter-rouge">KerasTrainer</code> and target feature extractor created in <a href="#classification_training">§4.2</a> for supervised training</li>
</ul>

<p>While it may not be immediately obvious from such a small example, TF-GNN helped at each step by providing not just the underlying operations we need to perform on graphs, but also many useful protocols and helper functions taking care of much of the boilerplate code we would have otherwise required. Using these in tandem with the orchestrator means that all of the components are easily extendable and/or replaceable. Moreover, it allows us at least in principle to easily scale the various moving parts independently and without unnecessary pain. For example, introducing a non-trivial strategy in the <a href="#strategy">trainer</a> we could distribute our training across multiple GPUs or, eventually, TPUs, while also parallelizing our input pipeline through the <code class="language-plaintext highlighter-rouge">InputContext</code> that is <a href="#input_context">passed on to the <code class="language-plaintext highlighter-rouge">DatasetProvider</code></a>.</p>

<hr />

<p>The results we obtained for the drug cardiotoxicity dataset are good, but not impressive. This was to be expected given the very simple GAT-based model that we implemented and the fact that we did no hyperparameter optimization or principled architectural choices. For comparison, we partially quote here <a href="#cardiotox">Table 1 from [1]</a>, where we see that our AUROC results are essentially consistent with the GNN baseline considered there:</p>

<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog/tfgnn-intro/table-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog/tfgnn-intro/table-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog/tfgnn-intro/table-1400.webp" />
    <!-- Fallback to the original file -->
    <!-- style="max-width:100%;max-height:100%" -->
    <img src="/assets/img/blog/tfgnn-intro/table.png" />

  </picture>

</figure>

<p>The reader is encouraged to try various modifications of the GNN architecture to improve the performance of the model, as well as study its behavior on the out-of-distribution test sets <code class="language-plaintext highlighter-rouge">test1_dataset_provider</code> and <code class="language-plaintext highlighter-rouge">test2_dataset_provider</code> that we prepared in <a href="#dataprep">§2</a> but did not use here.</p>

<div class="section separator"></div>

<h3 id="51-resources-and-acknowledgements"><a id="conclusions_resources">5.1 Resources and acknowledgements</a></h3>

<p>There are a number of resources that have inspired or been used in this notebook, some of which are cited along the way. They are also collected here for reference purposes:</p>

<ul>
  <li>
    <p>The paper <a href="#cardiotox">[1]</a> introducing the CardioTox dataset digs deeper in the data we have used, and builds better classification models for it</p>
  </li>
  <li>
    <p>The papers <a href="#gat">[2]</a> and <a href="#gat">[3]</a> introduced and popularized Graph Attention Networks</p>
  </li>
  <li>
    <p><a href="https://distill.pub/2021/gnn-intro/">A Gentle Introduction to Graph Neural Networks</a> is a good reference to start learning more about GNNs and their applications</p>
  </li>
  <li>
    <p>For those looking to dig deeper into GNNs, the <a href="https://www.cs.mcgill.ca/~wlh/grl_book/">Graph Representation Learning Book</a> by W. L. Hamilton is fairly up-to-date and more comprehensive</p>
  </li>
  <li>
    <p>The <a href="https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/intro.md">TensorFlow GNN guide</a> is a good place to start learning how to use TF-GNN, and some of this notebook’s code was inspired by the examples provided there</p>
  </li>
  <li>
    <p><strong>[July 5, 2022 UPDATE]</strong> Some official notebooks with TF-GNN examples are now available, see <em>e.g.</em> <a href="https://colab.research.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/intro_mutag_example.ipynb">Molecular Graph Classification</a>, <a href="https://colab.research.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb#scrollTo=udvGTpefWRE_">Solving OGBN-MAG end-to-end</a> and <a href="https://colab.research.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/graph_network_shortest_path.ipynb">Learning shortest paths with GraphNetworks</a></p>
  </li>
</ul>]]></content><author><name></name></author><category term="programming" /><category term="python" /><category term="graphs" /><category term="neural-networks" /><category term="tensorflow-gnn" /><category term="tensorflow-datasets" /><summary type="html"><![CDATA[A Graph Attention baseline for drug cardiotoxicity detection]]></summary></entry></feed>